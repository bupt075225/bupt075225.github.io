## web crawler/spider

链接管理

数据选择器

结构化数据管理

## Spider

Spiders是一个类，定义爬虫如何对指定站点进行数据抓取，怎样从页面中提取出结构化的数据。通过继承这个类，可以定制抓取数据和解析页面。

Spider的工作过程是首先向start_urls属性中定义的第一个url发送HTTP请求，等待响应返回；得到响应的同时回调parse()方法，该方法使用选择器提取数据并以Item的形式返回。

第一个HTTP请求通过调用`start_requests()`方法发起，parse()方法作为响应回调，用来解析页面，返回Item和Request对象。Request对象对应的响应回调可能也是parse()方法，但也可以是其它定制方法。

启动spider时调用`start_requests()`方法，这个方法可以override，例如在抓取动态页面需要处理JS时，借助splash就需要改写这个方法。

几种常用的Spider：

* CrawlSpider
* XMLFeedSpider
* CSVFeedSpider

## Item

爬虫的主要目的就是从非结构化的数据源中提取出结构化的数据，它提取到的原始信息是以列表方式输出。scrapy提供Item类来定义一个通用数据格式，实质是一个容器，提供类似字典的API。

## Item Loader

Item Loaders设计出来的目的是使选择器解析规则易于维护(没看出来！)。选择器提取到信息后放到Item Loader中，同一个item field可以收集一个或多个提取到的信息，Item Loader使用处理函数将它们拼接在一起。

使用Item Loader首先要实例化一个对象，使用类似字典的对象，如Item或dict进行初始化，也可以不显式提供，默认使用ItemLoader.default_item_class属性。

Item Loader为每个field提供了输入和输出处理函数，第一个参数必须是迭代器，输入处理函数的输出会保存在Item Loader内部一个列表中，输出处理函数的输出结果就是Item Field。输入和输出处理函数可以在Item Loader定义时进行声明，也可以在定义Item时通过field元数据声明，还可以使用默认的处理函数。

Item Loader Context是一个字典，所有item field输入和输出函数共享，用户来修改处理函数的行为(没看出来！)。当输入和输出函数接收Item Loader Context作为其中一个参数时，Item Loader会将当前的context传递给它。

## Item Pipeline

Item被抓取后，可以送到Item Pipeline，它通过几个组件依次处理Item，这些组件实质是实现了几个特定方法的类，它们接收Item并做相应处理，并决定是否传递到下一个组件，还是直接丢弃不再向下传递。Item Pipeline的使用场景通常是：Item数据验证、去重、写入数据库。

## Feed Exports

成功抓取到数据后，通常希望将数据进行持久化保存或以文件的形式导出，以供其它应用使用数据。Feed Exports功能是以items为数据源，提供多种序列化格式和存储方式。默认支持的序列化格式有xml、json、cvs，支持的存储方式包括本地文件系统、FTP、S3云存储、标准输出。

## Link Extractors

	from scrapy.linkextractors import LinkExtractor

`LinkExtractor`类从response中提取URL链接，extract_links()方法返回一个Link类的实例化对象，其中url属性存放了提取到的URL。

`LinkExtractor`类通常和CrawlSpider的Rule对象结合使用，用来跟踪页面链接并提取信息。当然也可以与其它类型spider结合使用，它的目的仅仅是用来提取URL链接。

## Settings

用来定制scrapy所有组件的行为，可以有多种方式进行配置设定。常用的设置如下：

	USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.94 Safari/537.36'
	DOWNLOAD_DELAY=3
	COOKIES_ENABLED=False

## 最佳实践

1.文件和图片下载

Files Pipeline和Images Pipeline用来处理文件和图片下载。Images Pipeline可以对下载的图形生成缩略图，转换成JPG/RGB格式，但依赖Pillow库，所以需要首先确认已安装Pillow。

Images Pipeline的工作过程如下：首先需要在spider中抓取图片URL地址，item中定义一个名为image_urls的域，用来存放抓到的URL；另外一个名为images的域用来存放图片信息。其次，需要配置images pipeline使能，下载存放路径。然后，spider将item传递到Images Pipeline,get_media_request(image_url)方法发起http请求，开始下载图片，它的优先级较低，会排在抓取页面信息之后。

2.xpath选择器

从html文档中提取指定的元素内容通常使用xpath，它的语法很简洁。

最简单的使用方法是相对路径写得足够长，总是能提取到想要的元素及元素的属性值：

	'//li/div/div/div/p/img/@src'

相对路径再结合[]选择条件和@属性，能解决大部分问题：

	'//div/div/p[@id="company-name"]'
	'//p[@id="company-number"]/strong'

3.持久化存储数据

4.跟踪链接



5.自动登录

## 禁止抓取的应对方法

* 在settings.py中设置USER_AGENT，伪装成浏览器。
* 在settings.py中设置对同一个网站的http请求延时属性DOWNLOAD_DELAY
* 在settings.py中设置禁止使用cookies属性COOKIES_ENABLED

如果上述措施仍然被禁，就考虑使用

* 使用动态IP(目前还没使用过)

## 问题

* 抓取到的中文是unicode编码，不能显示出中文字符
* 返回403错误码，禁止抓取网页

## 其它

Selenium是一个可以让浏览器自动化执行任务的工具，常用户于自动化测试。与Beautiful Soup或phantomJS等结合使用，也适合爬取动态网页数据。

## scrapy系统架构

![](http://i.imgur.com/dyZU3RO.png)

Downloader

负责抓取页面，提供给scrapy engine，最后传给spider做处理。

Downloader middlewares

位于scrapy engine和downloader之间的一系列钩子，负责处理engine发往downloader的请求，downloader发给engine的响应。它提供了一种机制来扩展scrapy的功能。

## 参考链接

1. [scrapy-splash](https://github.com/scrapy-plugins/scrapy-splash#why-not-use-the-splash-http-api-directly)
2. [Handling JavaScript in Scrapy with Splash](https://blog.scrapinghub.com/2015/03/02/handling-javascript-in-scrapy-with-splash/)
